# Unified LLM Interface - Product Requirements Document

## Project Overview
A **provider-agnostic LLM interface** that provides seamless switching between different LLM providers while maintaining a consistent API surface. The framework abstracts away provider-specific implementations and message formats, including advanced reasoning capabilities.

## Core Design Principles

### 1. **Provider Agnostic**
- Unified API that works identically across all providers
- Provider-specific logic isolated in implementation classes
- Easy addition of new providers without changing user code

### 2. **Standardized Interface**
- Single message format (OpenAI-compatible) for all providers
- Consistent method signatures: `chat()` and `chat_stream()`
- Transparent provider switching via import statements
- Unified reasoning support across different provider formats

### 3. **Extensible Architecture**
- Clear extension points for new providers
- Minimal core abstractions with provider-specific implementations
- Plugin-style architecture for easy provider additions

### 4. **Pure Interface Design**
- **Providers**: Translation layer only (no business logic or auto-execution)
- **ToolExecutor**: Optional utility for safe tool execution with error handling
- **Application**: Full control over conversation flow and tool execution decisions
- **Clear separation**: Interface responsibilities vs agent/application responsibilities

## Unified API Design

### Core Interface
```python
from unified_llm import ProviderA, ProviderB, ProviderC

# Same interface regardless of provider
provider = ProviderA(model_id="model-name", **config)
response = provider.chat(messages, tools=None, enable_reasoning=True)
stream = provider.chat_stream(messages, tools=None, enable_reasoning=True)
```

### Base Provider Contract
```python
class BaseProvider:
    def __init__(self, model_id: str, tools: list[Callable] = None, **config):
        """Initialize provider with model and configuration.
        
        Args:
            tools: Functions for schema generation only (NO auto-execution)
        """
        
    def chat(self, messages: list[dict], enable_reasoning: bool = False) -> ChatResponse:
        """Synchronous chat completion with optional reasoning.
        
        Returns:
            ChatResponse with tool_calls as data (not executed)
        """
        
    def chat_stream(self, messages: list[dict], enable_reasoning: bool = False) -> Iterator[ChatStreamResponse]:
        """Streaming chat completion with optional reasoning.
        
        Yields:
            ChatStreamResponse chunks with tool_calls as data (not executed)
        """
        
    # Provider-specific implementation methods (abstract)
    def _prepare_request(self, messages: list[dict], enable_reasoning: bool = False) -> dict:
        """Convert unified format to provider-specific format"""
        
    def _execute_request(self, request: dict) -> dict:
        """Execute provider-specific API call"""
        
    def _parse_response(self, response: dict, enable_reasoning: bool = False) -> ChatResponse:
        """Convert provider response to unified format (tool calls as data only)"""
        
    def _construct_tools(self, functions: list[Callable]) -> dict:
        """Convert functions to provider-specific tool schema format"""
        
    def _extract_reasoning_content(self, response: dict) -> tuple[str, str | None]:
        """Extract content and reasoning from provider response"""
```

## Reasoning Support Design

### Unified Reasoning Interface
The framework provides consistent reasoning support across all providers regardless of their native implementation:

```python
# Enable reasoning for any provider
provider = SomeProvider(model_id="reasoning-model")

response = provider.chat(
    messages=[{"role": "user", "content": "Solve: 9.11 vs 9.8, which is greater?"}],
    enable_reasoning=True
)

print("Final Answer:", response.content)
print("Reasoning Process:", response.reasoning_content)
```

### Provider-Specific Reasoning Handling

#### 1. **OpenAI (o1 models)**
- Handle `reasoning_tokens` in usage metadata
- Extract reasoning summaries when available
- Map to unified `reasoning_content` field

#### 2. **AWS Bedrock**  
- Use `reasoningConfig` parameter to enable reasoning
- Map `reasoning` field to `reasoning_content`
- Map `output` field to `content`

#### 3. **vLLM/Ollama**
- Use native `reasoning_content` support
- Handle `--reasoning-parser` configuration automatically
- Support thinking enable/disable parameters

#### 4. **Other Providers**
- Parse reasoning markers (`<think>`, `<reasoning>`, etc.) automatically
- Extract reasoning content from structured output
- Fall back to content-only when reasoning not supported

### Reasoning Response Format
```python
class ChatResponse(BaseModel):
    content: str                           # Final answer content
    reasoning_content: str | None = None   # Reasoning process (if available)
    reasoning_tokens: int | None = None    # Token count for reasoning (if provided)
    
class ChatStreamResponse(BaseModel):
    delta: str                             # Incremental final content
    reasoning_delta: str | None = None     # Incremental reasoning content
    is_reasoning_complete: bool = False    # Reasoning phase completion
    is_complete: bool = False              # Final response completion
```

### Streaming Reasoning Support
```python
provider = SomeProvider(model_id="reasoning-model")

for chunk in provider.chat_stream(messages, enable_reasoning=True):
    if chunk.reasoning_delta:
        print(f"Thinking: {chunk.reasoning_delta}", end="", flush=True)
    elif chunk.delta:
        if chunk.is_reasoning_complete:
            print("\nAnswer: ", end="", flush=True)
        print(chunk.delta, end="", flush=True)
```

### Reasoning with Structured Outputs
```python
from pydantic import BaseModel

class MathSolution(BaseModel):
    answer: float
    explanation: str

provider = SomeProvider(model_id="reasoning-model")
response = provider.chat(
    messages=[{"role": "user", "content": "What is 15% of 240?"}],
    enable_reasoning=True,
    response_format=MathSolution
)

print("Reasoning:", response.reasoning_content)  # Thought process
print("Structured Answer:", response.content)    # JSON following schema
```

### Reasoning with Tool Calls
```python
def calculate(expression: str) -> float:
    """Evaluate mathematical expressions safely."""
    return eval(expression)

provider = SomeProvider(
    model_id="reasoning-model",
    tools=[calculate]  # Schema generation only
)
tool_executor = ToolExecutor(tools=[calculate])

messages = [{"role": "user", "content": "Calculate 15% of 240 step by step"}]
response = provider.chat(messages, enable_reasoning=True)

print("Reasoning:", response.reasoning_content)  # Shows tool selection logic
print("Tool calls requested:", response.tool_calls)

if response.tool_calls:
    # Application handles tool execution
    messages.append({
        "role": "assistant",
        "content": response.content,
        "tool_calls": response.tool_calls
    })
    
    for tool_call in response.tool_calls:
        result = tool_executor.execute(tool_call)
        messages.append({
            "role": "tool",
            "content": result,
            "tool_call_id": tool_call["id"]
        })
    
    final_response = provider.chat(messages, enable_reasoning=True)
    print("Final Answer:", final_response.content)
```

## Message Format Standardization

### Unified Format (OpenAI-Compatible)
All providers accept and return messages in this format:

```python
# Input messages
messages = [
    {"role": "system", "content": "You are a helpful assistant"},
    {"role": "user", "content": "Hello"},
    {"role": "assistant", "content": "Hi there!"},
    {
        "role": "user", 
        "content": [
            {"type": "text", "text": "What's in this image?"},
            {"type": "image", "image_data": "base64..."}
        ]
    }
]

# Tool calling format
{
    "role": "assistant",
    "content": "I'll help you with that.",
    "tool_calls": [
        {"id": "call_1", "name": "function_name", "arguments": "{...}"}
    ]
}

# Tool results
{"role": "tool", "content": "result", "tool_call_id": "call_1"}
```

### Provider Format Abstraction
- **Framework responsibility**: Handle all format conversions internally
- **Provider implementations**: Convert between unified ↔ native formats
- **User isolation**: Users never see provider-specific message structures

## Tool Integration Design

### Design Philosophy: Interface vs Agent Separation

The framework follows a **pure interface approach** where:
- **Providers**: Translate formats, return tool calls as data (no auto-execution)
- **ToolExecutor**: Optional utility for executing tools with error handling  
- **Application**: Manages conversation history and orchestrates tool execution

### Function-Based Tool Definition
```python
def get_weather(location: str) -> str:
    """Get current weather for a location."""
    return f"Weather in {location}: sunny, 22°C"

def calculate(expression: str) -> float:
    """Evaluate a mathematical expression."""
    return eval(expression)

# Provider initialization with tools (for schema generation only)
provider = SomeProvider(
    model_id="model-name",
    tools=[get_weather, calculate]  # Used for schema conversion, NOT auto-execution
)
```

### Provider Tool Processing (Interface Only)
1. **Function Introspection**: Extract names, docstrings, and type hints automatically
2. **Schema Generation**: Convert to provider-specific tool schema format
3. **Response Parsing**: Return tool calls as structured data
4. **NO EXECUTION**: Providers never execute tools automatically

### ToolExecutor Utility (Optional)
```python
class ToolExecutor:
    """Simple utility for executing individual tool calls."""
    
    def __init__(self, tools: List[Callable]):
        self.tools_by_name = {tool.__name__: tool for tool in tools}
    
    def execute(self, tool_call: Dict[str, Any]) -> str:
        """Execute a single tool call with error handling.
        
        Returns:
            String result of tool execution (success or error message)
        """
        # Safe execution with proper error handling
```

### Application-Controlled Tool Flow
```python
# Setup
provider = SomeProvider(model_id="model", tools=[get_weather])
tool_executor = ToolExecutor(tools=[get_weather])  # Optional utility

# Pure interface usage
response = provider.chat(messages)
if response.tool_calls:
    print("Model wants to call:", response.tool_calls)
    # Application decides: execute, approve, log, etc.
    
    for tool_call in response.tool_calls:
        result = tool_executor.execute(tool_call)  # User choice to execute
        messages.append({
            "role": "tool", 
            "content": result, 
            "tool_call_id": tool_call["id"]
        })
    
    final_response = provider.chat(messages)  # Continue conversation
```

## Configuration Management

### Environment Variables (Optional)
```bash
# Provider-specific credentials
PROVIDER_A_API_KEY=xxx
PROVIDER_A_BASE_URL=xxx
PROVIDER_B_ENDPOINT=xxx
PROVIDER_B_REGION=xxx

# Common configuration
LLM_REQUEST_TIMEOUT=30
LLM_MAX_TOOL_CALLS=10
LLM_ENABLE_REASONING=true
```

### Direct Configuration (Primary)
```python
provider = SomeProvider(
    model_id="model-name",
    api_key="override-env-var",
    base_url="custom-endpoint",
    temperature=0.7,
    max_tokens=1000,
    tools=[func1, func2],
    enable_reasoning=True  # Global reasoning setting
)
```

## Data Models (Minimal Pydantic)

### Response Models Only
```python
class ChatResponse(BaseModel):
    content: str                           # Final response text
    reasoning_content: str | None = None   # Reasoning process (if available)
    reasoning_tokens: int | None = None    # Token count for reasoning
    
class ChatStreamResponse(BaseModel):
    delta: str                             # Incremental final content
    reasoning_delta: str | None = None     # Incremental reasoning content
    is_reasoning_complete: bool = False    # Reasoning phase complete
    is_complete: bool = False              # Stream completion flag
```

### Message Format
- **Native Python types**: `list[dict]` for messages
- **No complex models**: Keep data structures simple and flexible
- **Validation**: Basic type checking without over-engineering

## Provider Implementation Guide

### Adding a New Provider
1. **Inherit from BaseProvider**
2. **Implement required abstract methods**
3. **Handle provider-specific authentication**
4. **Convert message formats in `_prepare_request` and `_parse_response`**
5. **Map tool calling formats in `_construct_tools`**
6. **Implement reasoning extraction in `_extract_reasoning_content`**

### Provider Template
```python
class NewProvider(BaseProvider):
    def __init__(self, model_id: str, **config):
        self.client = initialize_provider_client(config)
        super().__init__(model_id, **config)
    
    def _prepare_request(self, messages: list[dict], enable_reasoning: bool = False) -> dict:
        # Convert unified format to provider format
        # Handle reasoning configuration if supported
        return converted_request
    
    def _execute_request(self, request: dict) -> dict:
        # Make provider API call
        return provider_response
    
    def _parse_response(self, response: dict, enable_reasoning: bool = False) -> ChatResponse:
        # Extract reasoning and content
        content, reasoning = self._extract_reasoning_content(response)
        return ChatResponse(content=content, reasoning_content=reasoning)
    
    def _extract_reasoning_content(self, response: dict) -> tuple[str, str | None]:
        # Provider-specific reasoning extraction logic
        return final_content, reasoning_content
    
    def _construct_tools(self, functions: list[Callable]) -> dict:
        # Convert functions to provider tool schema
        return provider_tool_schema
```

### Reasoning Implementation Requirements

#### For Each Provider
1. **Detection**: Identify if model supports reasoning
2. **Configuration**: Map enable_reasoning to provider-specific parameters
3. **Parsing**: Extract reasoning content from provider response format
4. **Streaming**: Handle incremental reasoning content in streams
5. **Error Handling**: Graceful fallback when reasoning not supported

#### Reasoning Pattern Mapping
```python
# Provider-specific reasoning extraction patterns
REASONING_PATTERNS = {
    "deepseek": {"start": "<think>", "end": "</think>"},
    "qwen": {"start": "<think>", "end": "</think>"},
    "granite": {"start": "<thinking>", "end": "</thinking>"},
    # Add patterns as needed, but avoid maintaining large registries
}
```

## Multimodal Support

### Image Handling
- **Standard format**: Base64 encoding only
- **Consistent interface**: Same content structure across providers
- **Provider conversion**: Handle different image format requirements internally
- **Reasoning with images**: Support visual reasoning across providers

### Content Block Structure
```python
{
    "role": "user",
    "content": [
        {"type": "text", "text": "Describe this image and explain your reasoning"},
        {"type": "image", "image_data": "base64_encoded_data"}
    ]
}
```

## Error Handling Strategy

### Graceful Degradation
- **Tool execution errors**: Return error message to model for handling
- **Network failures**: Clear error messages with retry suggestions
- **Invalid inputs**: Validation with helpful error descriptions
- **Provider-specific errors**: Normalize to common error format
- **Reasoning unavailable**: Fall back to content-only responses

### Error Categories
```python
class UnifiedLLMError(Exception):
    """Base exception for all framework errors"""

class ProviderError(UnifiedLLMError):
    """Provider-specific API errors"""

class ToolExecutionError(UnifiedLLMError):
    """Tool execution failures"""

class ValidationError(UnifiedLLMError):
    """Input validation errors"""

class ReasoningUnsupportedError(UnifiedLLMError):
    """Reasoning not supported by provider/model"""
```

## Performance Considerations

### Optimization Points
- **Tool schema caching**: Build schemas once during initialization
- **Connection pooling**: Reuse HTTP connections where possible
- **Streaming efficiency**: Minimize buffering in stream responses
- **Error handling**: Fast-fail validation before expensive API calls
- **Reasoning parsing**: Efficient pattern matching for reasoning extraction
- **Token counting**: Cache reasoning token counts when available

## Usage Examples

### Basic Chat
```python
from unified_llm import SomeProvider

provider = SomeProvider(model_id="model-name")
response = provider.chat([
    {"role": "user", "content": "Hello, world!"}
])
print(response.content)
```

### Reasoning Chat
```python
from unified_llm import SomeProvider

provider = SomeProvider(model_id="reasoning-model")
response = provider.chat(
    messages=[{"role": "user", "content": "Solve this step by step: 9.11 vs 9.8"}],
    enable_reasoning=True
)

if response.reasoning_content:
    print("Reasoning:", response.reasoning_content)
print("Answer:", response.content)
```

### Provider Switching
```python
# Switch providers without changing code
from unified_llm import ProviderA as Provider
# from unified_llm import ProviderB as Provider

provider = Provider(model_id="model-name", tools=[my_function])
response = provider.chat(messages, enable_reasoning=True)  # Identical interface
```

### Tool Usage with Reasoning
```python
def search_web(query: str) -> str:
    """Search the web for information."""
    return f"Search results for: {query}"

provider = SomeProvider(
    model_id="reasoning-model",
    tools=[search_web]  # Schema generation only
)
tool_executor = ToolExecutor(tools=[search_web])

messages = [{"role": "user", "content": "Find recent news about AI"}]
response = provider.chat(messages, enable_reasoning=True)

print("Reasoning:", response.reasoning_content)  # Shows tool selection logic

if response.tool_calls:
    # Application controls tool execution
    print("Tool calls requested:", response.tool_calls)
    
    # Add assistant message
    messages.append({
        "role": "assistant", 
        "content": response.content,
        "tool_calls": response.tool_calls
    })
    
    # Execute tools
    for tool_call in response.tool_calls:
        result = tool_executor.execute(tool_call)
        messages.append({
            "role": "tool",
            "content": result,
            "tool_call_id": tool_call["id"]
        })
    
    # Get final response
    final_response = provider.chat(messages, enable_reasoning=True)
    print("Final Answer:", final_response.content)
else:
    print("Direct Answer:", response.content)
```

## Implementation Phases

### Phase 1: Core Framework
- BaseProvider abstract class (without auto-execution methods)
- Message format standardization  
- Basic chat() implementation (returns tool calls as data)
- One reference provider implementation
- Remove `_handle_tool_calls` method (anti-pattern for pure interface)

### Phase 2: Essential Features
- Tool calling system with function introspection (schema generation only)
- ToolExecutor utility class for optional tool execution
- Streaming support (chat_stream)
- Error handling and validation
- Second provider implementation

### Phase 3: Advanced Features ⭐
- **Reasoning support (PRIORITY)**
  - Unified reasoning interface across providers
  - Provider-specific reasoning extraction
  - Streaming reasoning content
  - Reasoning + structured outputs
  - Reasoning + tool calling integration
- Multimodal support (images)
- Configuration management
- Performance optimizations

### Phase 4: Production Features
- Advanced error handling and retries
- Metrics and monitoring
- Caching and optimization
- Extended provider support

## Reasoning Implementation Tasks

### Core Reasoning Infrastructure
- [ ] Extend BaseProvider with reasoning methods
- [ ] Update ChatResponse/ChatStreamResponse models
- [ ] Implement reasoning content extraction utilities
- [ ] Add enable_reasoning parameter handling

### Provider-Specific Implementation
- [ ] OpenAI reasoning support (o1 models)
- [ ] AWS Bedrock reasoning integration  
- [ ] vLLM reasoning_content support
- [ ] Ollama thinking parameter support
- [ ] Generic pattern-based reasoning extraction

### Advanced Reasoning Features
- [ ] Streaming reasoning content
- [ ] Reasoning + structured outputs
- [ ] Reasoning + tool calling
- [ ] Reasoning token counting
- [ ] Performance optimization for reasoning

### Testing & Validation
- [ ] Unit tests for reasoning extraction
- [ ] Integration tests across providers
- [ ] Performance benchmarks
- [ ] Documentation and examples

## Success Criteria

### Developer Experience
- **Same code works** with any provider
- **Easy provider switching** via import changes
- **Pure interface design** with clear separation of concerns
- **Tool calls as data** - application controls execution
- **Clear error messages** for debugging
- **Consistent reasoning interface** regardless of provider

### Extensibility
- **New providers** can be added without changing existing code
- **Provider-specific features** can be accessed when needed
- **Framework evolution** doesn't break existing integrations
- **Reasoning support** easily extensible to new providers

### Production Ready
- **Robust error handling** for network and API failures
- **Efficient resource usage** for high-throughput scenarios
- **Clear logging** for debugging and monitoring
- **Reasoning performance** optimized for real-time use

---
